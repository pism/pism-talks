I hope by now, most if not all of you are aware that the recent generation of ice sheet models struggled with reproducing historical mass loss. Uncertainties in mass loss projections arise from uncertainties in model parameters, from model uncertainty, from the climate forcing, and from the initial state.

If you are interested in how we can improve initial states, I suggest to visit the poster of Annegret Pohle this afternoon.
In this talk, I'd like to introduce a probabilistic calibration method to ensure that model simulations agree observations. I'm neither the first to talk about Bayesian calibration for ice sheet models nor do I claim our method is particularly ingenious. Quite the contrary, I'm going to use basic concepts from Bayesian statistics and from Machine Learning that many of you are familiar with.

Calibrating sea-level projections using observations can greatly reduce the variance and correct the bias. Let's start with a prior. In this case we use the 500-member ensemble projections from our paper in 2019. This ensemble started in 2008. However, there is nothing special about our prior, any large ensemble can be calibrated.
The left column shows observed and simulated cumulative mass loss at 2020 in cm sea-level equivalent. For observations, we use the reconciled mass balance from the Ice Sheet Mass Balance Intercomparison Experiment (IMBIE). The mean is the solid black line, and the dashed black lines mark two standard deviations.
Our projections are grouped into the three RCP scenarios 2.6, 4.5, and 8.5. At 2020, the pathways have not yet diverged, and the projections are the same.
At the top of each plot the percentiles are shown. The solid black line is the median. Comparing the 5/95th percentiles of the projections to the two sigmas of the observations, we note the large variance and the large bias. Our prior projections over-estimate cumulative mass loss at 2020.
Now if ww look at the posteriors, which are the calibrated projections, we note that both bias and variance are greatly reduced. Observed mean and posterior median agree well.
How does this translate into projections at 2100?
Again the uncalibrated projections show a large variance. After calibration, the posterior projections show less variance and the median is lower, too.

How does our calibration work?
Our goal is to calculate the probability of a contribution to sea-level by 2100, denoted here by Delta 2100, given data D, that is, observations, an ice sheet model H, and climate forcing F. We can split this into a projection and a calibration period. We will focus on calibration and use a two step method. First we will calibrate some model parameters and in the second step we will use particle filtering to cull an ensemble of simulations. What I like about this approach is that we do not need to assign weigths to each observational data set.

Which targets should be use for calibration? A natural choice is the quantity of interest. In our case, we want to predict mass loss, so we use mass change.

Another good target are surface speeds because uncertainties in ice flow parameters contribute significantly to uncertainties in future mass loss.

The workflow is the following. First we need an ice sheet model. For now we are using the Parallel Ice Sheet Model or PISM. Again nothing special about PISM, and any other ice sheet model could be used. We then generate training data with PISM. To do this, we selected 8 parameters which stronglhy control how ice flow. We drew 1,000 samples from this 8-dimensional parameter space, and ran PISM 1,000 times, once for each parameter combination. This gives us 1,000 surface speeds fields.

We then used those surface speeds fields to train a surrogate model. Or surrogate models, to be more precise.

To reduce the dimensionality of the problem, we performed a principal component analysis. This yields a set of "eigenglaciers". It's pretty similar to calculating eigenfaces in image analysis.

Then we trained a 4 layer Neural Network on the principal components. To reduce overfitting, we actually created 50 surrogate models with slighlty different weights to form a committee. The model is implemented in PyTorch-Lightning and can be trained on GPUs.

For all parameters, we set Beta priors and then sample the parameter space given observed surface speeds. Here we use a manifold Metropolis Adjusted Languevin Algorithm with a variable step size. This can also be done on GPUs. This gives us the  posterior distributions of our parameters.

Then we created a new ensemble by drawing 500 samples from the posterior distributions and ran PISM with these 500 samples. The new "flow" calibrated simulations show much better agreement with observations.

The second calibration step is the particle filtering, also known as Bayesian calibration or importance sampling. Here we weigh members based on their likelihood relative to observations of mass change. Then we resample simulations proportionally to the likelihood to create a new ensemble that is consistent with respect to both the observations of surface speed and mass change.

This fully calibrated ensemble now produced mass loss in agreement with observations, which is by design.

Taking that fully calibrated ensemble and running it out to 2100 brings us back to the slide I showed earlier.
We find that compared to the uncalibrated prior simulations, the variance is reduced as well as the bias.

To make sure that the calibrated ensemble does not agree with observations for the completely wrong reasons, we also compared the ensemble's ice discharge and surface mass balance to IMBIE. And in the calibrated case, both agree well.
